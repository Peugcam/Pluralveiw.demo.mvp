import { createClient } from '@supabase/supabase-js'
import OpenAI from 'openai'
import { tavily } from '@tavily/core'
import { temporalDetector } from '../../lib/temporalDetector'

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
)

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

const tavilyClient = tavily({ apiKey: process.env.TAVILY_API_KEY })

// Cache em mem√≥ria para buscas similares (15 minutos de TTL)
const searchCache = new Map()
const CACHE_TTL = 15 * 60 * 1000 // 15 minutos

// Fun√ß√£o para gerar chave de cache
function getCacheKey(topic, perspectiveName, perspectiveFocus) {
  return `${topic.toLowerCase().trim()}:${perspectiveName}:${perspectiveFocus}`.replace(/\s+/g, '_')
}

// Fun√ß√£o para limpar cache expirado
function cleanExpiredCache() {
  const now = Date.now()
  for (const [key, value] of searchCache.entries()) {
    if (now - value.timestamp > CACHE_TTL) {
      searchCache.delete(key)
    }
  }
}

export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  try {
    const { topic } = req.body

    if (!topic) {
      return res.status(400).json({ error: 'Topic is required' })
    }

    // üïê DETECTAR TERMOS TEMPORAIS NA QUERY
    const temporalInfo = temporalDetector.detect(topic)

    if (temporalInfo && temporalInfo.detected) {
      console.log(`[Temporal] Detectado: "${temporalInfo.label}" - ${temporalDetector.formatDateRange(temporalInfo)}`)
      console.log(`[Temporal] Query original: "${topic}"`)
      console.log(`[Temporal] Query aprimorada: "${temporalInfo.enhancedQuery}"`)
    }

    // Criar an√°lise no banco (sem user_id para MVP)
    const { data: analysis, error: analysisError } = await supabase
      .from('analyses')
      .insert({
        topic: topic,
        status: 'processing'
      })
      .select()
      .single()

    if (analysisError) throw analysisError

    // Gerar as 6 perspectivas com IA, passando informa√ß√£o temporal
    const perspectives = await generatePerspectives(topic, temporalInfo)

    // Salvar perspectivas no banco
    const perspectivesData = perspectives.map(p => ({
      analysis_id: analysis.id,
      type: p.type,
      content: p.content,
      sources: p.sources
    }))

    const { error: perspectivesError } = await supabase
      .from('perspectives')
      .insert(perspectivesData)

    if (perspectivesError) throw perspectivesError

    // Gerar perguntas reflexivas
    const questions = await generateReflectiveQuestions(topic, perspectives)

    // Salvar perguntas no banco
    const questionsData = questions.map(q => ({
      analysis_id: analysis.id,
      question: q
    }))

    const { error: questionsError } = await supabase
      .from('reflective_questions')
      .insert(questionsData)

    if (questionsError) throw questionsError

    // Atualizar status da an√°lise
    await supabase
      .from('analyses')
      .update({ status: 'completed' })
      .eq('id', analysis.id)

    // Preparar resposta com informa√ß√£o temporal se dispon√≠vel
    const response = {
      success: true,
      analysisId: analysis.id,
      perspectives,
      questions
    }

    // üïê ADICIONAR INFORMA√á√ÉO TEMPORAL NA RESPOSTA
    if (temporalInfo && temporalInfo.detected) {
      response.temporalInfo = {
        detected: true,
        type: temporalInfo.type,
        label: temporalInfo.label,
        dateRange: temporalDetector.formatDateRange(temporalInfo),
        startDate: temporalInfo.startDate.toISOString(),
        endDate: temporalInfo.endDate.toISOString()
      }
    }

    res.status(200).json(response)

  } catch (error) {
    console.error('Error in analyze API:', error)
    console.error('Error details:', {
      message: error.message,
      stack: error.stack,
      name: error.name
    })
    res.status(500).json({
      error: error.message || 'Unknown error',
      details: process.env.NODE_ENV === 'development' ? error.stack : undefined
    })
  }
}

// Fun√ß√£o para filtrar fontes relevantes usando IA
async function filterRelevantSources(sources, topic, perspectiveFocus) {
  try {
    // Filtrar fontes em lote para economia de tokens
    const sourcesText = sources.map((s, idx) =>
      `[${idx}] T√≠tulo: ${s.title}\nConte√∫do: ${s.content.substring(0, 300)}...`
    ).join('\n\n')

    const prompt = `Analise quais das seguintes fontes s√£o RELEVANTES para uma an√°lise sobre "${topic}" com foco em "${perspectiveFocus}".

FONTES:
${sourcesText}

Responda APENAS com os n√∫meros das fontes relevantes separados por v√≠rgula (ex: 0,2,4).
Se nenhuma for relevante, responda "nenhuma".`

    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0,
      max_tokens: 50
    })

    const relevantIndices = response.choices[0].message.content.trim().toLowerCase()

    if (relevantIndices === 'nenhuma') {
      return sources.slice(0, 3) // Retornar as 3 primeiras se nenhuma for considerada relevante
    }

    const indices = relevantIndices.split(',').map(i => parseInt(i.trim())).filter(i => !isNaN(i))
    return indices.map(i => sources[i]).filter(Boolean)
  } catch (error) {
    console.error('Error filtering relevant sources:', error)
    return sources.slice(0, 6) // Em caso de erro, retornar as primeiras
  }
}

async function searchRealSources(topic, perspectiveName, perspectiveFocus, temporalInfo = null) {
  try {
    // Verificar cache primeiro
    const cacheKey = getCacheKey(topic, perspectiveName, perspectiveFocus)
    const cached = searchCache.get(cacheKey)

    if (cached && (Date.now() - cached.timestamp < CACHE_TTL)) {
      console.log(`[Cache HIT] ${perspectiveName} - usando resultados em cache`)
      // Mesmo com cache, aplicar filtro temporal se necess√°rio
      if (temporalInfo && temporalInfo.detected) {
        const filteredData = {
          ...cached.data,
          sources: cached.data.sources // Fontes j√° foram filtradas quando salvas no cache
        }
        return filteredData
      }
      return cached.data
    }

    console.log(`[Cache MISS] ${perspectiveName} - buscando na web`)

    // Limpar cache expirado periodicamente
    if (searchCache.size > 50) {
      cleanExpiredCache()
    }

    // üïê USAR QUERY APRIMORADA SE HOUVER INFORMA√á√ÉO TEMPORAL
    let searchQuery = `${topic} ${perspectiveName} ${perspectiveFocus}`

    if (temporalInfo && temporalInfo.detected) {
      // Usar a query aprimorada que inclui contexto de data
      searchQuery = `${temporalInfo.enhancedQuery} ${perspectiveName} ${perspectiveFocus}`
      console.log(`[Temporal Search] ${perspectiveName}: "${searchQuery}"`)
    }

    const searchResult = await tavilyClient.search(searchQuery, {
      maxResults: 15, // Buscar mais resultados para compensar filtros
      includeAnswer: false,
      includeRawContent: false
    })

    let results = searchResult.results || []

    // üïê FILTRAR RESULTADOS POR DATA SE TEMPORAL INFO PRESENTE
    if (temporalInfo && temporalInfo.detected) {
      const originalCount = results.length
      results = results.filter(result => temporalDetector.validateResult(result, temporalInfo))
      console.log(`[Temporal Filter] ${perspectiveName}: ${originalCount} resultados ‚Üí ${results.length} ap√≥s filtro temporal`)

      // Se filtrou demais, avisar
      if (results.length < 3) {
        console.warn(`[Temporal Filter] ${perspectiveName}: Poucos resultados ap√≥s filtro temporal (${results.length}). Dados podem ser limitados.`)
      }
    }

    // Filtrar fontes relevantes usando IA
    const relevantResults = await filterRelevantSources(results, topic, perspectiveFocus)

    // Categorizar resultados relevantes por tipo de dom√≠nio
    const categorizedSources = {
      institucional: [],
      academico: [],
      video: [],
      midia: []
    }

    relevantResults.forEach(result => {
      const url = result.url.toLowerCase()
      const source = {
        title: result.title,
        url: result.url,
        content: result.content
      }

      // Institucional: gov, org, institui√ß√µes
      if (url.includes('.gov') || url.includes('.org') || url.includes('governo') || url.includes('institution')) {
        categorizedSources.institucional.push(source)
      }
      // Acad√™mico: edu, scielo, scholar, universidades
      else if (url.includes('.edu') || url.includes('scholar') || url.includes('scielo') || url.includes('universidade') || url.includes('academic')) {
        categorizedSources.academico.push(source)
      }
      // V√≠deo: YouTube
      else if (url.includes('youtube.com') || url.includes('youtu.be')) {
        categorizedSources.video.push(source)
      }
      // M√≠dia: jornais, revistas, blogs de not√≠cias
      else {
        categorizedSources.midia.push(source)
      }
    })

    // Selecionar 1 de cada tipo (ou o que estiver dispon√≠vel)
    const finalSources = []

    if (categorizedSources.institucional.length > 0) {
      const s = categorizedSources.institucional[0]
      finalSources.push({ type: 'institucional', title: s.title, url: s.url })
    }

    if (categorizedSources.academico.length > 0) {
      const s = categorizedSources.academico[0]
      finalSources.push({ type: 'academico', title: s.title, url: s.url })
    }

    if (categorizedSources.video.length > 0) {
      const s = categorizedSources.video[0]
      finalSources.push({ type: 'video', title: s.title, url: s.url })
    }

    if (categorizedSources.midia.length > 0) {
      const s = categorizedSources.midia[0]
      finalSources.push({ type: 'midia', title: s.title, url: s.url })
    }

    // Se n√£o conseguiu pelo menos 3 fontes, pegar os primeiros resultados relevantes
    if (finalSources.length < 3 && relevantResults.length > 0) {
      const remaining = relevantResults.slice(0, 4 - finalSources.length)
      remaining.forEach(r => {
        finalSources.push({
          type: 'midia',
          title: r.title,
          url: r.url
        })
      })
    }

    // Aumentar contexto com mais fontes relevantes e informa√ß√µes estruturadas
    const contextSources = relevantResults.slice(0, 6).map(r => ({
      title: r.title,
      content: r.content,
      url: r.url
    }))

    const searchContext = contextSources
      .map(r => `üìÑ Fonte: ${r.title}\nURL: ${r.url}\nConte√∫do: ${r.content}`)
      .join('\n\n---\n\n')

    const result = {
      sources: finalSources,
      searchContext: searchContext
    }

    // Salvar no cache
    searchCache.set(cacheKey, {
      data: result,
      timestamp: Date.now()
    })

    console.log(`[Cache SAVE] ${perspectiveName} - resultados salvos no cache`)

    return result

  } catch (error) {
    console.error('Error searching sources:', error)
    return { sources: [], searchContext: '' }
  }
}

// Fun√ß√£o para validar alinhamento entre conte√∫do gerado e fontes
async function validateSourceAlignment(content, sources, topic) {
  try {
    if (!sources || sources.length === 0) {
      return { aligned: true, score: 100, needsImprovement: false }
    }

    const sourceTitles = sources.map(s => s.title).join('\n')

    const prompt = `Analise se o texto abaixo est√° alinhado e cita/referencia as fontes fornecidas.

TEXTO GERADO:
${content}

FONTES DISPON√çVEIS:
${sourceTitles}

AVALIE:
1. O texto menciona ou cita informa√ß√µes das fontes?
2. As fontes s√£o relevantes para o que foi escrito?
3. Score de alinhamento (0-100)

Responda APENAS em formato JSON:
{"aligned": true/false, "score": 0-100, "reason": "breve explica√ß√£o"}`

    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0,
      max_tokens: 150
    })

    const result = JSON.parse(response.choices[0].message.content)

    return {
      aligned: result.score >= 60,
      score: result.score,
      reason: result.reason,
      needsImprovement: result.score < 60
    }
  } catch (error) {
    console.error('Error validating source alignment:', error)
    return { aligned: true, score: 100, needsImprovement: false } // Em caso de erro, prosseguir
  }
}

async function generatePerspectives(topic, temporalInfo = null) {
  const perspectiveTypes = [
    { type: 'tecnica', name: 'T√©cnica', focus: 'aspectos t√©cnicos, dados, evid√™ncias cient√≠ficas' },
    { type: 'popular', name: 'Popular', focus: 'senso comum, impacto no dia a dia das pessoas' },
    { type: 'institucional', name: 'Institucional', focus: 'posi√ß√£o de institui√ß√µes, √≥rg√£os oficiais, governos' },
    { type: 'academica', name: 'Acad√™mica', focus: 'teorias, pesquisas, vis√£o cient√≠fica e universit√°ria' },
    { type: 'conservadora', name: 'Conservadora', focus: 'tradi√ß√£o, valores conservadores, cautela com mudan√ßas' },
    { type: 'progressista', name: 'Progressista', focus: 'mudan√ßa social, inova√ß√£o, justi√ßa e equidade' }
  ]

  // Gerar todas as perspectivas em paralelo
  const perspectivesPromises = perspectiveTypes.map(async (pt) => {
    // Buscar fontes reais na web, passando o foco para busca mais precisa E informa√ß√£o temporal
    const { sources, searchContext } = await searchRealSources(topic, pt.name, pt.focus, temporalInfo)

    // üïê ADICIONAR CONTEXTO TEMPORAL AO PROMPT SE DISPON√çVEL
    let temporalContext = ''
    if (temporalInfo && temporalInfo.detected) {
      temporalContext = `
‚è∞ IMPORTANTE - CONTEXTO TEMPORAL:
Esta consulta refere-se especificamente a: ${temporalInfo.label}
Per√≠odo: ${temporalDetector.formatDateRange(temporalInfo)}
Data atual: ${temporalDetector.formatDate(new Date())}

INSTRU√á√ïES TEMPORAIS:
- Foque APENAS em informa√ß√µes deste per√≠odo espec√≠fico
- Se os dados n√£o forem recentes o suficiente, mencione isso explicitamente
- Priorize fontes com datas dentro do per√≠odo solicitado
- N√ÉO use informa√ß√µes desatualizadas ou de per√≠odos diferentes
`
    }

    // Gerar an√°lise baseada no conte√∫do real encontrado
    const prompt = `Voc√™ √© um analista especializado em an√°lise de m√∫ltiplas perspectivas.

TEMA EXATO A SER ANALISADO: "${topic}"
${temporalContext}
PERSPECTIVA A ANALISAR: ${pt.name}
FOCO: ${pt.focus}

CONTEXTO DE FONTES REAIS ENCONTRADAS:
${searchContext || 'Nenhum contexto espec√≠fico encontrado. Use seu conhecimento geral.'}

INSTRU√á√ïES IMPORTANTES:
- Mantenha o foco EXCLUSIVAMENTE no tema "${topic}"
- N√ÉO fuja do tema ou fa√ßa generaliza√ß√µes amplas
- Seja espec√≠fico sobre "${topic}" sob a perspectiva ${pt.name}
- **CITE EXPLICITAMENTE as fontes fornecidas acima quando relevantes**
- Use express√µes como: "Segundo [fonte]...", "Estudos indicam que...", "De acordo com...", "Dados mostram que..."
- Integre naturalmente informa√ß√µes e dados das fontes no texto
- Se mencionar um dado ou fato espec√≠fico, referencie a fonte de onde veio
- Escreva 2-3 par√°grafos focados e bem fundamentados
- Priorize informa√ß√µes das fontes fornecidas sobre conhecimento geral

FORMATO DE CITA√á√ÉO:
- Use cita√ß√µes naturais integradas ao texto (n√£o use numera√ß√£o ou lista de refer√™ncias)
- Exemplo: "Estudos recentes demonstram que..." ou "Segundo an√°lise da [institui√ß√£o]..."

RESPONDA APENAS com a an√°lise, SEM t√≠tulo ou introdu√ß√£o.`

    const contentCompletion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content: 'Voc√™ √© um analista imparcial especializado em fornecer an√°lises focadas e objetivas sobre temas espec√≠ficos.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.5,
      max_tokens: 600
    })

    const generatedContent = contentCompletion.choices[0].message.content

    // Validar alinhamento entre conte√∫do e fontes
    const validation = await validateSourceAlignment(generatedContent, sources, topic)

    // Log para monitoramento de qualidade
    console.log(`[${pt.name}] Alinhamento: ${validation.score}/100 - ${validation.reason || 'OK'}`)

    // Se o alinhamento for muito baixo, adicionar aviso no log (mas n√£o bloquear)
    if (validation.needsImprovement) {
      console.warn(`[${pt.name}] ‚ö†Ô∏è Baixo alinhamento com fontes. Considere revisar.`)
    }

    return {
      type: pt.type,
      content: generatedContent,
      sources: sources,
      alignmentScore: validation.score // Adicionar score para an√°lise futura
    }
  })

  const perspectives = await Promise.all(perspectivesPromises)
  return perspectives
}

async function generateReflectiveQuestions(topic, perspectives) {
  const perspectivesText = perspectives.map(p =>
    `${p.type}: ${p.content.substring(0, 200)}...`
  ).join('\n\n')

  const prompt = `Baseado nestas perspectivas sobre "${topic}":

${perspectivesText}

Gere 5 perguntas reflexivas que:
1. Estimulem pensamento cr√≠tico
2. Conectem diferentes perspectivas
3. Incentivem o leitor a formar sua pr√≥pria opini√£o
4. Sejam abertas (sem resposta certa/errada)

Formato: Uma pergunta por linha, sem numera√ß√£o.`

  const completion = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: prompt }],
    temperature: 0.8,
    max_tokens: 300
  })

  const questions = completion.choices[0].message.content
    .split('\n')
    .filter(q => q.trim().length > 0)
    .slice(0, 5)

  return questions
}
